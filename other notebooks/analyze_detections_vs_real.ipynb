{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Detection Results - Compare Detections vs Real Calls\n",
    "\n",
    "This notebook helps you:\n",
    "1. Load detection results from CSV files\n",
    "2. Extract audio segments from detected time ranges\n",
    "3. Visualize mel-spectrograms of detections\n",
    "4. Compare side-by-side: **Detected samples vs Real primate calls**\n",
    "5. Identify false positives for hard negative mining\n",
    "\n",
    "**Goal**: Understand what the model is detecting and whether they are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q librosa soundfile matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.append('src')  # Adjust to your path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import config\n",
    "import data_loader\n",
    "import preprocessing\n",
    "\n",
    "config.print_config_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all detection CSV files\n",
    "detection_dir = config.DETECTION_OUTPUT_DIR\n",
    "\n",
    "detection_files = [f for f in os.listdir(detection_dir) if f.endswith('_detections.csv')]\n",
    "\n",
    "print(f\"Found {len(detection_files)} detection files:\")\n",
    "for i, f in enumerate(detection_files):\n",
    "    print(f\"  {i}: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which detection file to analyze\n",
    "file_idx = 0  # Change this to select different file\n",
    "\n",
    "detection_csv = os.path.join(detection_dir, detection_files[file_idx])\n",
    "detections_df = pd.read_csv(detection_csv)\n",
    "\n",
    "# Get corresponding audio file name\n",
    "audio_filename = detection_files[file_idx].replace('_detections.csv', '.wav')\n",
    "audio_path = os.path.join(config.LONG_AUDIO_ROOT, audio_filename)\n",
    "\n",
    "print(f\"\\nAnalyzing: {audio_filename}\")\n",
    "print(f\"Total detections: {len(detections_df)}\")\n",
    "print(f\"\\nDetection summary:\")\n",
    "print(detections_df.head(10))\n",
    "\n",
    "if len(detections_df) > 0:\n",
    "    print(f\"\\nBy species:\")\n",
    "    print(detections_df['species'].value_counts())\n",
    "    \n",
    "    print(f\"\\nConfidence statistics:\")\n",
    "    print(detections_df.groupby('species')['confidence'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Uncertain Detections (Potential False Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on medium-confidence detections (0.5 - 0.85)\n",
    "# These are most likely to be false positives\n",
    "\n",
    "min_conf = 0.5\n",
    "max_conf = 0.85\n",
    "\n",
    "uncertain_df = detections_df[\n",
    "    (detections_df['confidence'] >= min_conf) & \n",
    "    (detections_df['confidence'] <= max_conf)\n",
    "].copy()\n",
    "\n",
    "uncertain_df = uncertain_df.sort_values('confidence', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Uncertain detections ({min_conf}-{max_conf} confidence): {len(uncertain_df)}\")\n",
    "print(f\"\\nBy species:\")\n",
    "if len(uncertain_df) > 0:\n",
    "    print(uncertain_df['species'].value_counts())\n",
    "    print(f\"\\nTop 10 uncertain detections:\")\n",
    "    print(uncertain_df.head(10))\n",
    "else:\n",
    "    print(\"No uncertain detections found. Try adjusting confidence range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Real Primate Calls (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real primate samples\n",
    "print(\"Loading real primate call samples...\\n\")\n",
    "\n",
    "species_data = data_loader.load_species_data()\n",
    "\n",
    "# Store a few examples from each species\n",
    "real_samples = {}\n",
    "\n",
    "for species_name, audio_list in species_data.items():\n",
    "    # Take first 3 samples as references\n",
    "    real_samples[species_name] = [\n",
    "        {\n",
    "            'audio': audio,\n",
    "            'filepath': filepath,\n",
    "            'spec': preprocessing.audio_to_melspectrogram(audio)\n",
    "        }\n",
    "        for audio, filepath in audio_list[:3]\n",
    "    ]\n",
    "    \n",
    "    print(f\"{species_name}: Loaded {len(real_samples[species_name])} reference samples\")\n",
    "\n",
    "print(\"\\nReal samples ready for comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detection_with_real(detection_row, audio_path, real_samples_dict, play_audio=True):\n",
    "    \"\"\"\n",
    "    Compare a detection with real primate calls\n",
    "    \n",
    "    Shows:\n",
    "    - Left: Detected segment from long audio\n",
    "    - Right: Real primate call of the same species\n",
    "    \"\"\"\n",
    "    species = detection_row['species']\n",
    "    start_time = detection_row['start_time']\n",
    "    end_time = detection_row['end_time']\n",
    "    confidence = detection_row['confidence']\n",
    "    \n",
    "    # Load long audio and extract segment\n",
    "    print(f\"Loading segment from {os.path.basename(audio_path)}...\")\n",
    "    full_audio, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE)\n",
    "    \n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    detected_audio = full_audio[start_sample:end_sample]\n",
    "    \n",
    "    # Compute detected spectrogram\n",
    "    detected_spec = preprocessing.audio_to_melspectrogram(detected_audio, sr)\n",
    "    \n",
    "    # Get a real sample of the same species\n",
    "    if species in real_samples_dict and len(real_samples_dict[species]) > 0:\n",
    "        real_sample = real_samples_dict[species][0]  # Take first reference\n",
    "        real_audio = real_sample['audio']\n",
    "        real_spec = real_sample['spec']\n",
    "        real_filepath = real_sample['filepath']\n",
    "    else:\n",
    "        print(f\"âš ï¸  No real samples available for {species}\")\n",
    "        real_audio = None\n",
    "        real_spec = None\n",
    "        real_filepath = None\n",
    "    \n",
    "    # Create figure\n",
    "    if real_spec is not None:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    fig.suptitle(\n",
    "        f\"DETECTION vs REAL CALL - {species}\\n\" +\n",
    "        f\"Detection: {start_time:.1f}-{end_time:.1f}s, Confidence: {confidence:.3f}\",\n",
    "        fontsize=16, fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    # ===== LEFT COLUMN: DETECTED SEGMENT =====\n",
    "    \n",
    "    # Detected waveform\n",
    "    times = np.arange(len(detected_audio)) / sr\n",
    "    axes[0, 0].plot(times, detected_audio, linewidth=0.5, color='red', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Time (s)')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    axes[0, 0].set_title(f'DETECTED - Waveform\\n(from long audio at {start_time:.1f}s)', \n",
    "                         fontsize=12, fontweight='bold', color='red')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Detected spectrogram\n",
    "    img1 = librosa.display.specshow(\n",
    "        detected_spec,\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        sr=sr,\n",
    "        hop_length=config.HOP_LENGTH,\n",
    "        fmin=config.FMIN,\n",
    "        fmax=config.FMAX,\n",
    "        ax=axes[1, 0],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[1, 0].set_title(f'DETECTED - Mel-Spectrogram', \n",
    "                         fontsize=12, fontweight='bold', color='red')\n",
    "    fig.colorbar(img1, ax=axes[1, 0], format='%+2.0f dB')\n",
    "    \n",
    "    \n",
    "    if real_spec is not None:\n",
    "        # Real waveform\n",
    "        times_real = np.arange(len(real_audio)) / sr\n",
    "        axes[0, 1].plot(times_real, real_audio, linewidth=0.5, color='green', alpha=0.8)\n",
    "        axes[0, 1].set_xlabel('Time (s)')\n",
    "        axes[0, 1].set_ylabel('Amplitude')\n",
    "        axes[0, 1].set_title(f'REAL {species} - Waveform\\n(training sample)', \n",
    "                            fontsize=12, fontweight='bold', color='green')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Real spectrogram\n",
    "        img2 = librosa.display.specshow(\n",
    "            real_spec,\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            sr=sr,\n",
    "            hop_length=config.HOP_LENGTH,\n",
    "            fmin=config.FMIN,\n",
    "            fmax=config.FMAX,\n",
    "            ax=axes[1, 1],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        axes[1, 1].set_title(f'REAL {species} - Mel-Spectrogram', \n",
    "                            fontsize=12, fontweight='bold', color='green')\n",
    "        fig.colorbar(img2, ax=axes[1, 1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Audio playback\n",
    "    if play_audio:\n",
    "        print(\"\\nðŸ”Š Audio Playback:\")\n",
    "        print(\"\\nDETECTED segment:\")\n",
    "        display(Audio(detected_audio, rate=sr))\n",
    "        \n",
    "        if real_audio is not None:\n",
    "            print(f\"\\nREAL {species} call (for comparison):\")\n",
    "            display(Audio(real_audio, rate=sr))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Uncertain Detections One by One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze first 10 uncertain detections\n",
    "n_to_analyze = min(10, len(uncertain_df))\n",
    "\n",
    "print(f\"Analyzing {n_to_analyze} uncertain detections...\\n\")\n",
    "\n",
    "for i in range(n_to_analyze):\n",
    "\n",
    "    print(f\"# DETECTION {i+1}/{n_to_analyze}\")\n",
    "\n",
    "    \n",
    "    detection = uncertain_df.iloc[i]\n",
    "    \n",
    "    compare_detection_with_real(\n",
    "        detection,\n",
    "        audio_path,\n",
    "        real_samples,\n",
    "        play_audio=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Specific Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific detection to examine in detail\n",
    "detection_idx = 0  # Change this number\n",
    "\n",
    "if len(uncertain_df) > detection_idx:\n",
    "    detection = uncertain_df.iloc[detection_idx]\n",
    "    \n",
    "    \n",
    "    compare_detection_with_real(\n",
    "        detection,\n",
    "        audio_path,\n",
    "        real_samples,\n",
    "        play_audio=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index {detection_idx} out of range (total: {len(uncertain_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Real Samples with One Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detection_with_multiple_real(detection_row, audio_path, real_samples_dict):\n",
    "    \"\"\"\n",
    "    Show one detection alongside multiple real samples\n",
    "    \"\"\"\n",
    "    species = detection_row['species']\n",
    "    start_time = detection_row['start_time']\n",
    "    end_time = detection_row['end_time']\n",
    "    confidence = detection_row['confidence']\n",
    "    \n",
    "    # Load detected segment\n",
    "    full_audio, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE)\n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    detected_audio = full_audio[start_sample:end_sample]\n",
    "    detected_spec = preprocessing.audio_to_melspectrogram(detected_audio, sr)\n",
    "    \n",
    "    # Get real samples\n",
    "    if species not in real_samples_dict:\n",
    "        print(f\"No real samples for {species}\")\n",
    "        return\n",
    "    \n",
    "    real_samples_list = real_samples_dict[species][:3]  # Take up to 3\n",
    "    \n",
    "    # Create figure\n",
    "    n_real = len(real_samples_list)\n",
    "    fig, axes = plt.subplots(2, n_real + 1, figsize=(6*(n_real+1), 10))\n",
    "    \n",
    "    fig.suptitle(\n",
    "        f\"Detection vs Multiple Real {species} Calls\\n\" +\n",
    "        f\"Confidence: {confidence:.3f}\",\n",
    "        fontsize=16, fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    # Column 0: Detected\n",
    "    times = np.arange(len(detected_audio)) / sr\n",
    "    axes[0, 0].plot(times, detected_audio, linewidth=0.5, color='red')\n",
    "    axes[0, 0].set_title('DETECTED', fontweight='bold', color='red', fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    librosa.display.specshow(\n",
    "        detected_spec, x_axis='time', y_axis='mel',\n",
    "        sr=sr, hop_length=config.HOP_LENGTH,\n",
    "        fmin=config.FMIN, fmax=config.FMAX,\n",
    "        ax=axes[1, 0], cmap='viridis'\n",
    "    )\n",
    "    axes[1, 0].set_title(f'at {start_time:.1f}s', fontsize=10)\n",
    "    \n",
    "    # Columns 1+: Real samples\n",
    "    for i, sample in enumerate(real_samples_list, 1):\n",
    "        real_audio = sample['audio']\n",
    "        real_spec = sample['spec']\n",
    "        \n",
    "        times_real = np.arange(len(real_audio)) / sr\n",
    "        axes[0, i].plot(times_real, real_audio, linewidth=0.5, color='green')\n",
    "        axes[0, i].set_title(f'REAL #{i}', fontweight='bold', color='green', fontsize=12)\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        librosa.display.specshow(\n",
    "            real_spec, x_axis='time', y_axis='mel',\n",
    "            sr=sr, hop_length=config.HOP_LENGTH,\n",
    "            fmin=config.FMIN, fmax=config.FMAX,\n",
    "            ax=axes[1, i], cmap='viridis'\n",
    "        )\n",
    "        axes[1, i].set_title(f'{os.path.basename(sample[\"filepath\"])[:20]}...', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare one detection with 3 real samples\n",
    "if len(uncertain_df) > 0:\n",
    "    detection_idx = 0  # Change this\n",
    "    \n",
    "    compare_detection_with_multiple_real(\n",
    "        uncertain_df.iloc[detection_idx],\n",
    "        audio_path,\n",
    "        real_samples\n",
    "    )\n",
    "else:\n",
    "    print(\"No uncertain detections to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis: Show All Uncertain Detections (Spectrograms Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of all uncertain detections\n",
    "n_to_show = min(20, len(uncertain_df))\n",
    "\n",
    "if n_to_show > 0:\n",
    "    print(f\"Showing spectrograms of {n_to_show} uncertain detections...\\n\")\n",
    "    \n",
    "    # Load long audio once\n",
    "    full_audio, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE)\n",
    "    \n",
    "    # Create grid\n",
    "    n_cols = 5\n",
    "    n_rows = int(np.ceil(n_to_show / n_cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_to_show):\n",
    "        detection = uncertain_df.iloc[i]\n",
    "        \n",
    "        # Extract segment\n",
    "        start_sample = int(detection['start_time'] * sr)\n",
    "        end_sample = int(detection['end_time'] * sr)\n",
    "        segment = full_audio[start_sample:end_sample]\n",
    "        \n",
    "        # Compute spectrogram\n",
    "        spec = preprocessing.audio_to_melspectrogram(segment, sr)\n",
    "        \n",
    "        # Plot\n",
    "        librosa.display.specshow(\n",
    "            spec,\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            sr=sr,\n",
    "            hop_length=config.HOP_LENGTH,\n",
    "            fmin=config.FMIN,\n",
    "            fmax=config.FMAX,\n",
    "            ax=axes[i],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        \n",
    "        axes[i].set_title(\n",
    "            f\"#{i}: {detection['species']}\\n\" +\n",
    "            f\"{detection['start_time']:.1f}s, conf={detection['confidence']:.2f}\",\n",
    "            fontsize=9\n",
    "        )\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_to_show, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    fig.suptitle(\n",
    "        f\"All Uncertain Detections - {os.path.basename(audio_path)}\",\n",
    "        fontsize=16,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLook for patterns:\")\n",
    "    print(\"  - Do some spectrograms look very different from real calls?\")\n",
    "    print(\"  - Are there common visual features in false positives?\")\n",
    "    print(\"  - Do bird calls have a distinctive pattern?\")\n",
    "else:\n",
    "    print(\"No uncertain detections to show\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
