{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Model Errors - EDA\n",
    "\n",
    "This notebook helps you understand **why the model makes mistakes** by:\n",
    "1. Loading a trained model\n",
    "2. Running predictions on validation or detection data\n",
    "3. Finding misclassified samples\n",
    "4. Visualizing their mel-spectrograms and waveforms\n",
    "\n",
    "**Use this to:**\n",
    "- Identify common error patterns\n",
    "- See what sounds confuse the model\n",
    "- Decide what hard negatives to collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q librosa soundfile tensorflow matplotlib\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.append('/content')  # Adjust if your src/ is elsewhere\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import config\n",
    "import data_loader\n",
    "import preprocessing\n",
    "import model as model_module\n",
    "\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Analyze Validation Set Errors\n",
    "\n",
    "Use this to see errors on clean training data (helps understand if model learned properly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = os.path.join(config.MODEL_SAVE_DIR, 'best_model.h5')\n",
    "model = model_module.load_trained_model(model_path)\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare validation data\n",
    "# (You'll need to recreate the train/val split to get the same validation set)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Loading data...\")\n",
    "species_data = data_loader.load_species_data()\n",
    "background_data = data_loader.load_background_data()\n",
    "\n",
    "# Convert to spectrograms\n",
    "print(\"\\nConverting to spectrograms...\")\n",
    "species_specs = {}\n",
    "species_files = {}  # Keep track of file paths\n",
    "\n",
    "for species_name, audio_list in species_data.items():\n",
    "    specs = []\n",
    "    files = []\n",
    "    for audio, filepath in audio_list:\n",
    "        spec = preprocessing.audio_to_melspectrogram(audio)\n",
    "        specs.append(spec)\n",
    "        files.append(filepath)\n",
    "    species_specs[species_name] = specs\n",
    "    species_files[species_name] = files\n",
    "\n",
    "background_specs = []\n",
    "background_files = []\n",
    "for audio, filepath in background_data:\n",
    "    spec = preprocessing.audio_to_melspectrogram(audio)\n",
    "    background_specs.append(spec)\n",
    "    background_files.append(filepath)\n",
    "\n",
    "print(\"\\nPreparing dataset...\")\n",
    "# Combine all data\n",
    "X_all = []\n",
    "y_all = []\n",
    "file_paths = []\n",
    "labels_text = []\n",
    "\n",
    "label_map = {name: i for i, name in enumerate(config.CLASS_NAMES[:-1])}\n",
    "label_map['Background'] = len(label_map)\n",
    "\n",
    "# Add species data (NO AUGMENTATION for this EDA)\n",
    "for species_name, specs in species_specs.items():\n",
    "    for spec, filepath in zip(specs, species_files[species_name]):\n",
    "        X_all.append(spec)\n",
    "        y_all.append(label_map[species_name])\n",
    "        file_paths.append(filepath)\n",
    "        labels_text.append(species_name)\n",
    "\n",
    "# Add background\n",
    "for spec, filepath in zip(background_specs, background_files):\n",
    "    X_all.append(spec)\n",
    "    y_all.append(label_map['Background'])\n",
    "    file_paths.append(filepath)\n",
    "    labels_text.append('Background')\n",
    "\n",
    "X_all = np.array(X_all)\n",
    "y_all = np.array(y_all)\n",
    "\n",
    "print(f\"Total samples: {len(X_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val (same as training)\n",
    "X_train, X_val, y_train, y_val, files_train, files_val, labels_train, labels_val = train_test_split(\n",
    "    X_all, y_all, file_paths, labels_text,\n",
    "    test_size=config.VALIDATION_SPLIT,\n",
    "    random_state=config.RANDOM_SEED,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Class distribution:\")\n",
    "for i, class_name in enumerate(config.CLASS_NAMES):\n",
    "    count = np.sum(y_val == i)\n",
    "    print(f\"  {class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data for model input\n",
    "print(\"Preprocessing validation data...\")\n",
    "X_val_images = []\n",
    "\n",
    "for spec in X_val:\n",
    "    spec_norm = preprocessing.normalize_spectrogram(spec)\n",
    "    spec_resized = preprocessing.resize_spectrogram(spec_norm)\n",
    "    rgb = preprocessing.spectrogram_to_rgb(spec_resized)\n",
    "    model_input = preprocessing.preprocess_for_model(rgb)\n",
    "    X_val_images.append(model_input)\n",
    "\n",
    "X_val_images = np.array(X_val_images)\n",
    "print(f\"Shape: {X_val_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "print(\"Running predictions...\")\n",
    "predictions = model.predict(X_val_images, batch_size=32, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Find errors\n",
    "errors_mask = y_pred != y_val\n",
    "error_indices = np.where(errors_mask)[0]\n",
    "\n",
    "print(f\"\\nTotal validation samples: {len(y_val)}\")\n",
    "print(f\"Errors: {len(error_indices)}\")\n",
    "print(f\"Accuracy: {(1 - len(error_indices)/len(y_val)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error analysis dataframe\n",
    "error_data = []\n",
    "\n",
    "for idx in error_indices:\n",
    "    true_label = y_val[idx]\n",
    "    pred_label = y_pred[idx]\n",
    "    confidence = predictions[idx, pred_label]\n",
    "    \n",
    "    error_data.append({\n",
    "        'index': idx,\n",
    "        'file': os.path.basename(files_val[idx]),\n",
    "        'true_class': config.CLASS_NAMES[true_label],\n",
    "        'predicted_class': config.CLASS_NAMES[pred_label],\n",
    "        'confidence': confidence,\n",
    "        'true_label_num': true_label,\n",
    "        'pred_label_num': pred_label\n",
    "    })\n",
    "\n",
    "errors_df = pd.DataFrame(error_data)\n",
    "errors_df = errors_df.sort_values('confidence', ascending=False)\n",
    "\n",
    "print(\"\\nError Summary:\")\n",
    "print(errors_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion patterns\n",
    "print(\"\\nMost common confusion patterns:\")\n",
    "confusion_patterns = errors_df.groupby(['true_class', 'predicted_class']).size().reset_index(name='count')\n",
    "confusion_patterns = confusion_patterns.sort_values('count', ascending=False)\n",
    "print(confusion_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Error Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_error_sample(idx, show_waveform=True):\n",
    "    \"\"\"\n",
    "    Visualize an error sample: waveform + spectrogram + prediction probabilities\n",
    "    \"\"\"\n",
    "    error_row = errors_df[errors_df['index'] == idx].iloc[0]\n",
    "    \n",
    "    # Load audio\n",
    "    filepath = files_val[idx]\n",
    "    audio = data_loader.load_audio_file(filepath)\n",
    "    \n",
    "    # Get spectrogram\n",
    "    spec = X_val[idx]\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_probs = predictions[idx]\n",
    "    \n",
    "    # Create figure\n",
    "    if show_waveform:\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1])\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax2 = fig.add_subplot(gs[1, :])\n",
    "        ax3 = fig.add_subplot(gs[2, :])\n",
    "    else:\n",
    "        fig, (ax2, ax3) = plt.subplots(2, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Title\n",
    "    fig.suptitle(\n",
    "        f\"ERROR SAMPLE: {error_row['file']}\\n\" +\n",
    "        f\"True: {error_row['true_class']} | Predicted: {error_row['predicted_class']} \" +\n",
    "        f\"(confidence: {error_row['confidence']:.3f})\",\n",
    "        fontsize=14, fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    # Plot waveform\n",
    "    if show_waveform:\n",
    "        times = np.arange(len(audio)) / config.SAMPLE_RATE\n",
    "        ax1.plot(times, audio, linewidth=0.5, color='blue')\n",
    "        ax1.set_xlabel('Time (s)')\n",
    "        ax1.set_ylabel('Amplitude')\n",
    "        ax1.set_title('Waveform')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    img = librosa.display.specshow(\n",
    "        spec,\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        sr=config.SAMPLE_RATE,\n",
    "        hop_length=config.HOP_LENGTH,\n",
    "        fmin=config.FMIN,\n",
    "        fmax=config.FMAX,\n",
    "        ax=ax2,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax2.set_title('Mel-Spectrogram')\n",
    "    fig.colorbar(img, ax=ax2, format='%+2.0f dB')\n",
    "    \n",
    "    # Plot prediction probabilities\n",
    "    colors = ['green' if i == error_row['true_label_num'] else \n",
    "              'red' if i == error_row['pred_label_num'] else 'gray' \n",
    "              for i in range(len(config.CLASS_NAMES))]\n",
    "    \n",
    "    bars = ax3.bar(config.CLASS_NAMES, pred_probs, color=colors, alpha=0.7)\n",
    "    ax3.set_ylabel('Probability')\n",
    "    ax3.set_title('Model Predictions (Green=True label, Red=Predicted label)')\n",
    "    ax3.set_ylim([0, 1])\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, prob in zip(bars, pred_probs):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{prob:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed info\n",
    "    print(f\"\\nFile: {filepath}\")\n",
    "    print(f\"True class: {error_row['true_class']}\")\n",
    "    print(f\"Predicted: {error_row['predicted_class']} (confidence: {error_row['confidence']:.3f})\")\n",
    "    print(f\"\\nAll probabilities:\")\n",
    "    for i, (class_name, prob) in enumerate(zip(config.CLASS_NAMES, pred_probs)):\n",
    "        marker = \"<- TRUE\" if i == error_row['true_label_num'] else \"<- PRED\" if i == error_row['pred_label_num'] else \"\"\n",
    "        print(f\"  {class_name:30s}: {prob:.4f} {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 10 errors (highest confidence wrong predictions)\n",
    "print(\"Visualizing top 10 most confident errors...\\n\")\n",
    "\n",
    "for i, row in errors_df.head(10).iterrows():\n",
    "    visualize_error_sample(row['index'])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Analyze Detection Errors from Long Audio\n",
    "\n",
    "Use this to see false positives from real-world detection (more relevant for hard negative mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a long audio file\n",
    "long_audio_files = data_loader.get_long_audio_files()\n",
    "\n",
    "print(f\"Available long audio files: {len(long_audio_files)}\")\n",
    "for i, f in enumerate(long_audio_files[:5]):\n",
    "    print(f\"  {i}: {os.path.basename(f)}\")\n",
    "\n",
    "# Choose one\n",
    "audio_idx = 0  # Change this to select different file\n",
    "audio_path = long_audio_files[audio_idx]\n",
    "\n",
    "print(f\"\\nSelected: {os.path.basename(audio_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and detect\n",
    "import detection\n",
    "\n",
    "print(\"Running detection...\")\n",
    "detections_df = detection.detect_in_long_audio(\n",
    "    model,\n",
    "    audio_path,\n",
    "    confidence_threshold=0.5  # Lower threshold to see more detections\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(detections_df)} detections\")\n",
    "if len(detections_df) > 0:\n",
    "    print(\"\\nDetection summary:\")\n",
    "    print(detections_df.groupby('species')['confidence'].agg(['count', 'mean', 'min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on medium-confidence detections (likely false positives)\n",
    "if len(detections_df) > 0:\n",
    "    # Filter detections with confidence between 0.5 and 0.85\n",
    "    uncertain_detections = detections_df[\n",
    "        (detections_df['confidence'] >= 0.5) & \n",
    "        (detections_df['confidence'] <= 0.85)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nUncertain detections (0.5-0.85 confidence): {len(uncertain_detections)}\")\n",
    "    print(uncertain_detections.head(20))\n",
    "else:\n",
    "    print(\"No detections to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection_sample(audio_path, start_time, end_time, species, confidence):\n",
    "    \"\"\"\n",
    "    Visualize a detected segment from long audio\n",
    "    \"\"\"\n",
    "    # Load full audio\n",
    "    full_audio, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE)\n",
    "    \n",
    "    # Extract segment\n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    segment = full_audio[start_sample:end_sample]\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    spec = preprocessing.audio_to_melspectrogram(segment, sr)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 8))\n",
    "    \n",
    "    fig.suptitle(\n",
    "        f\"Detection at {start_time:.1f}-{end_time:.1f}s | \" +\n",
    "        f\"Predicted: {species} (confidence: {confidence:.3f})\",\n",
    "        fontsize=14, fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    # Waveform\n",
    "    times = np.arange(len(segment)) / sr\n",
    "    ax1.plot(times, segment, linewidth=0.5, color='blue')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    ax1.set_title('Waveform')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spectrogram\n",
    "    img = librosa.display.specshow(\n",
    "        spec,\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        sr=sr,\n",
    "        hop_length=config.HOP_LENGTH,\n",
    "        fmin=config.FMIN,\n",
    "        fmax=config.FMAX,\n",
    "        ax=ax2,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax2.set_title('Mel-Spectrogram')\n",
    "    fig.colorbar(img, ax=ax2, format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"File: {os.path.basename(audio_path)}\")\n",
    "    print(f\"Time: {start_time:.1f} - {end_time:.1f} seconds\")\n",
    "    print(f\"Predicted species: {species}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(\"\\nIs this a correct detection? (Look at the spectrogram)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertain detections (potential false positives)\n",
    "if len(uncertain_detections) > 0:\n",
    "    print(f\"Visualizing {min(10, len(uncertain_detections))} uncertain detections...\\n\")\n",
    "    \n",
    "    for i, row in uncertain_detections.head(10).iterrows():\n",
    "        visualize_detection_sample(\n",
    "            audio_path,\n",
    "            row['start_time'],\n",
    "            row['end_time'],\n",
    "            row['species'],\n",
    "            row['confidence']\n",
    "        )\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "else:\n",
    "    print(\"No uncertain detections to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive: Visualize Specific Detection by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific detection to examine\n",
    "detection_idx = 0  # Change this number to look at different detections\n",
    "\n",
    "if len(detections_df) > detection_idx:\n",
    "    row = detections_df.iloc[detection_idx]\n",
    "    visualize_detection_sample(\n",
    "        audio_path,\n",
    "        row['start_time'],\n",
    "        row['end_time'],\n",
    "        row['species'],\n",
    "        row['confidence']\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index {detection_idx} out of range (total detections: {len(detections_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Insights\n",
    "\n",
    "After running this notebook, you should be able to answer:\n",
    "\n",
    "1. **What types of sounds does the model confuse?**\n",
    "   - Bird calls that look like primate calls?\n",
    "   - Certain frequency patterns?\n",
    "\n",
    "2. **Are there visual patterns in error spectrograms?**\n",
    "   - Do they have similar frequency ranges?\n",
    "   - Similar temporal patterns?\n",
    "\n",
    "3. **What should go into hard negatives?**\n",
    "   - Uncertain detections that are clearly NOT primate calls\n",
    "   - Sounds with medium confidence (0.5-0.85)\n",
    "\n",
    "4. **Is the model's confusion systematic?**\n",
    "   - Does it always confuse Species A with Species B?\n",
    "   - Does it misclassify background as a specific species?\n",
    "\n",
    "Use these insights to:\n",
    "- Collect better hard negatives\n",
    "- Add more training data for confused classes\n",
    "- Adjust preprocessing parameters (frequency range, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
